{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06422b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "from PyPDF2 import PdfReader\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64938c0f",
   "metadata": {},
   "source": [
    "### Script Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a86f7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_content(pdf_path):\n",
    "    pdf_content = PdfReader(pdf_path)\n",
    "\n",
    "    extracted_text = \"\"\n",
    "    for page in pdf_content.pages:\n",
    "        extracted_text += page.extract_text()\n",
    "\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90b6ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt):\n",
    "    client = InferenceClient(\n",
    "        provider=\"novita\",\n",
    "        api_key=os.environ[\"HF_TOKEN_READ\"],\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"deepseek-ai/DeepSeek-R1-0528\",\n",
    "        messages=prompt,\n",
    "    )\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2fb42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder_path = \"../data\"\n",
    "in_file_name = \"research_paper.pdf\"\n",
    "in_file_path = f\"{in_folder_path}/{in_file_name}\"\n",
    "extracted_content = get_pdf_content(in_file_path)\n",
    "\n",
    "podcast_question_gen_prompt = [\n",
    "                                {\n",
    "                                    \"role\": \"system\",\n",
    "                                    \"content\": \"\"\"Generate 5 thoughtful and engaging questions for a podcast, based only on the core topics from the provided research paper.\n",
    "\n",
    "                                                        The audience includes both technical professionals and business managers. So, write the questions in a clear, jargon-free way—balancing accuracy with simplicity.\n",
    "\n",
    "                                                        Focus on the main contributions, methods, or challenges highlighted in the paper. Questions should spark discussion and make the topic approachable for a wide audience.\"\"\",\n",
    "                                },\n",
    "                                {\"role\": \"user\", \"content\": extracted_content},\n",
    "                            ]\n",
    "raw_podcast_questions = get_llm_response(podcast_question_gen_prompt)\n",
    "podcast_questions = (\n",
    "    raw_podcast_questions\n",
    "    .choices[0]\n",
    "    .message.content.split(\"</think>\")[-1]\n",
    ")\n",
    "\n",
    "podcast_script_gen_prompt = [\n",
    "                                {\n",
    "                                    \"role\": \"system\",\n",
    "                                    \"content\": f\"\"\"Generate a podcast script based on the research paper content provided by the user.\n",
    "\n",
    "                                    Use the following structure:\n",
    "                                    - Introduction: Briefly introduce the research topic in simple, layman-friendly language.\n",
    "                                    - Core Discussion: Using the questions below, generate clear and concise answers derived from the user content. Maintain a conversational and engaging tone, suitable for both technical professionals and non-experts.\n",
    "\n",
    "                                    Questions:\n",
    "                                    {podcast_questions}\n",
    "\n",
    "                                    - Conclusion: Summarize the conversation and conclude the podcast script.\n",
    "\n",
    "                                    Ensure the entire script is easy to understand, avoids heavy jargon, and flows like a real podcast episode. Format the script as a monologue podcast where a single speaker walks the audience through the questions and answers.\"\"\"\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": extracted_content\n",
    "                                }\n",
    "                            ]\n",
    "\n",
    "raw_podcast_script = get_llm_response(podcast_script_gen_prompt)\n",
    "final_podcast_script = raw_podcast_script.choices[0].message.content.split(\"</think>\")[-1]\n",
    "\n",
    "out_folder_path = \"../transcripts\"\n",
    "out_file_name = in_file_name.split(\".\")[0] + \".txt\"\n",
    "out_folder_path = f\"{out_folder_path}/{out_file_name}\"\n",
    "\n",
    "with open(out_folder_path, \"w\") as f:\n",
    "    f.write(final_podcast_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e531a",
   "metadata": {},
   "source": [
    "#### Exploring the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c10b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Thought Process ----\n",
      "<think>\n",
      "Hmm, the user wants me to generate podcast questions based on the \"Attention Is All You Need\" paper for a mixed technical and business audience. Let me analyze the paper's core contributions first.\n",
      "\n",
      "The paper introduces the Transformer architecture which replaces RNNs/CNNs entirely with attention mechanisms. Key innovations include multi-head attention, positional encodings, and the encoder-decoder structure. The results show significant improvements in translation quality (28.4 BLEU for EN-DE) with much faster training times (3.5 days vs weeks).\n",
      "\n",
      "For the podcast format, I need to avoid technical jargon while keeping questions accurate. The audience includes both engineers and managers, so questions should bridge technical concepts with business impact. \n",
      "\n",
      "First question could focus on the fundamental shift - asking why attention alone suffices instead of RNNs. Second question should explain multi-head attention simply using the \"different perspectives\" analogy. Third question must address the positional encoding challenge since Transformers lack recurrence. \n",
      "\n",
      "For business relevance, the fourth question should highlight training efficiency gains - that 8 GPUs for 3.5 days outperformed models training for weeks. Finally, the last question could explore wider applications beyond translation, like the parsing results in section 6.3.\n",
      "\n",
      "The figures showing attention heads capturing syntax (like \"making...difficult\") are fascinating but too technical for this context. Better to keep questions high-level. Also need to emphasize how these speed improvements enable real-world deployments.\n",
      "\n",
      "Should I mention computational complexity comparisons from Table 1? Probably not - the O(1) vs O(n) detail might lose non-technical listeners. Focus instead on concrete outcomes like \"12 hours vs weeks\" from the abstract.\n",
      "\n",
      "The regularization techniques (label smoothing, dropout) are important but not core enough for introductory questions. Stick to architectural innovations and results. Positional embeddings vs encodings (Table 3 row E) had identical results - worth noting but not questioning since sinusoids worked fine.\n",
      "\n",
      "Most importantly: every question must pass the \"manager test\" - would someone non-technical grasp why this matters? Hence framing like \"why was this surprising\" and \"what real-world advantages\" throughout.\n",
      "\n",
      "\n",
      "---- Actual Output ----\n",
      "\n",
      "Here are 5 podcast questions based on the Transformer paper, designed for both technical and business audiences:\n",
      "\n",
      "1. **The Core Idea**: The paper claims \"attention is all you need.\" Traditionally, machine translation used complex systems with encoders, decoders, and recurrent networks. What made you realize that *attention mechanisms alone* could replace these entire systems? Why was this such a breakthrough?\n",
      "\n",
      "2. **Multi-Head Attention**: You introduce \"multi-head attention\" as a key component. In simple terms, how does allowing the model to focus on different parts of the input simultaneously (like having multiple \"perspectives\") improve its understanding compared to a single focus point?\n",
      "\n",
      "3. **Handling Sequence Order**: Since Transformers don't use recurrent networks that naturally process sequences step-by-step, how did you solve the challenge of representing the *order* of words? What's the intuition behind using those specific sine and cosine positional encodings?\n",
      "\n",
      "4. **Speed & Cost Advantage**: Your results show Transformers train much faster and cheaper than previous state-of-the-art models – achieving better results in just 3.5 days on 8 GPUs versus weeks on more hardware. What architectural choices specifically enable this massive gain in efficiency and parallelization?\n",
      "\n",
      "5. **Beyond Translation**: While focused on machine translation, you briefly show Transformers work well for English parsing too. What makes you believe this architecture could generalize to many other sequence-based tasks? What potential applications beyond language excite you most?\n"
     ]
    }
   ],
   "source": [
    "print(\"---- Thought Process ----\")\n",
    "print(raw_podcast_questions.choices[0].message.content.split(\"</think>\")[0])\n",
    "print(\"\\n---- Actual Output ----\")\n",
    "print(podcast_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a950869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Thought Process ----\n",
      "<think>\n",
      "Alright, let me tackle this podcast script request about the \"Attention Is All You Need\" paper. The user wants a conversational yet informative monologue covering five specific questions, avoiding heavy jargon while keeping it engaging for both technical and non-technical audiences. \n",
      "\n",
      "First, I need to distill the paper's essence: it introduced the Transformer architecture that relies solely on attention mechanisms, ditching RNNs/CNNs. The key innovations are multi-head attention, positional encoding, and the encoder-decoder structure. The paper demonstrates superior translation results with faster training times.\n",
      "\n",
      "For the introduction, I'll start with a relatable analogy about language processing to hook listeners. Then I'll address each question systematically:\n",
      "1) For \"why attention is all you need,\" I'll contrast sequential processing in RNNs with Transformer's parallel attention, using the \"spotlight\" metaphor from the paper's visualizations.\n",
      "2) Explaining multi-head attention requires emphasizing how different heads learn diverse relationships (like grammar vs context), avoiding matrix math explanations.\n",
      "3) The speed benefits tie directly to parallelization - this is where I'll highlight the business impact using their training time comparisons (hours vs weeks).\n",
      "4) For generalization beyond translation, the constituency parsing results are perfect evidence of architectural flexibility.\n",
      "5) The future implications section should acknowledge current limitations (like quadratic attention cost) while connecting to real-world applications like ChatGPT.\n",
      "\n",
      "Throughout, I'll replace terms like \"BLEU score\" with \"translation quality\" and skip encoder-decoder specifics. The conclusion will reinforce how this foundation changed AI landscape. The tone should stay enthusiastic but grounded - like explaining to a curious friend over coffee.\n",
      "\n",
      "\n",
      "---- Actual Output ----\n",
      "\n",
      "## Podcast Script: The Transformer Revolution - Attention Is All You Need\n",
      "\n",
      "**Host:** Hey everyone, and welcome to AI Unpacked! Today, we're diving into one of the most influential papers in modern AI: \"Attention Is All You Need\" by Vaswani, Shazeer, Parmar, and the team at Google Brain. This 2017 paper didn't just improve machine translation—it reshaped artificial intelligence. Let's break down why.\n",
      "\n",
      "**(Intro Music Fades)**\n",
      "\n",
      "---\n",
      "\n",
      "### Introduction  \n",
      "Imagine you're trying to understand a complex sentence. Your brain doesn't read it letter by letter—it jumps between key words, connects ideas across the sentence, and focuses on what matters most. That’s essentially what \"attention\" means in AI. Before 2017, systems like Google Translate relied on neural networks that processed words sequentially (like a slow conveyor belt), making them slow and resource-heavy. This paper asked: *What if we ditch the conveyor belt entirely and just use \"attention\"—the ability to focus on all parts of a sentence at once?* The result was the **Transformer**, the architecture behind ChatGPT, Gemini, and more. Let’s explore how it works.\n",
      "\n",
      "---\n",
      "\n",
      "### Core Discussion  \n",
      "\n",
      "#### 1. **The Core Innovation**  \n",
      "*How did focusing solely on attention replace essential components like recurrent layers?*  \n",
      "Traditional models processed words one after another, creating bottlenecks. The Transformer’s breakthrough was treating a sentence as a *web of relationships*. Instead of forcing the system to crawl through each word in order, **attention mechanisms** let it instantly link any word to any other word—like spotlighting all related words simultaneously. For example, to translate \"The cat sat on the mat,\" it connects \"sat\" directly to \"mat\" without passing through every intermediate word. This eliminated the need for slow, sequential components (RNNs), enabling parallel processing—like replacing a single-file line with a team working all at once.\n",
      "\n",
      "#### 2. **Multi-Head Attention**  \n",
      "*Why split attention into multiple \"heads\"?*  \n",
      "A single \"attention spotlight\" might miss nuances. **Multi-head attention** deploys multiple spotlights in parallel, each learning different types of relationships. Think of it like a committee: one head focuses on grammar (e.g., linking verbs to subjects), another on context (e.g., \"bank\" could mean river or money), and another on long-range dependencies (e.g., connecting \"it\" to a noun 20 words back). By combining these perspectives, the model captures richer meaning than a single head ever could. The paper showed 8 heads worked best—like having eight specialists collaborating.\n",
      "\n",
      "#### 3. **Speed & Practical Impact**  \n",
      "*What made Transformers faster and cheaper to train?*  \n",
      "Two words: **parallel processing**. Older models were like assembly lines—each step waited for the previous one. Transformers process all words simultaneously. The paper trained a model in *3.5 days* on 8 GPUs, compared to *weeks* for older systems. For businesses, this meant:  \n",
      "- **Lower costs**: Less computing power needed.  \n",
      "- **Scalability**: Handle larger datasets efficiently.  \n",
      "- **Real-time applications**: Faster training enables tools like real-time translators or chatbots.\n",
      "\n",
      "#### 4. **Beyond Translation**  \n",
      "*How did it generalize to other tasks like English parsing?*  \n",
      "The authors tested Transformers on **English constituency parsing** (diagramming sentence structure)—a task unrelated to translation. With minimal tweaks, it matched state-of-the-art systems. Why? Because attention is task-agnostic. Whether translating or parsing, the core need is the same: understanding relationships between words. This flexibility made Transformers the \"Swiss Army knife\" of AI—usable for text summarization, code generation, or even image recognition.\n",
      "\n",
      "#### 5. **Future Implications**  \n",
      "*What made this foundational for generative AI, and what limitations remain?*  \n",
      "Transformers unlocked **generative AI** because they handle context brilliantly. For instance, ChatGPT uses attention to track conversation history. But limitations noted in 2017 still persist:  \n",
      "- **Computational cost**: Attention scales poorly with long texts (e.g., books).  \n",
      "- **Data hunger**: They need massive datasets.  \n",
      "- **Black-box nature**: Hard to interpret why certain attention patterns form.  \n",
      "Still, the paper’s vision was spot-on—Transformers now process audio (Whisper), images (ViTs), and video.\n",
      "\n",
      "---\n",
      "\n",
      "### Conclusion  \n",
      "So, what’s the big takeaway? \"Attention Is All You Need\" proved that by focusing on *how words relate*, not the order they’re processed, AI could leap forward in speed, versatility, and power. It moved us from niche translation tools to systems that write, code, and create. While challenges like efficiency remain, this paper’s core idea—that relationships trump sequence—truly changed the game. Next time you ask ChatGPT a question, remember: it’s paying \"attention\" to every word you say.  \n",
      "\n",
      "Thanks for listening! If you enjoyed this, share it with a friend. See you next time on AI Unpacked.\n",
      "\n",
      "**(Outro Music Rises)**  \n",
      "\n",
      "---\n",
      "\n",
      "**Script Notes**:  \n",
      "- **Length**: ~4-5 minutes (adjustable by adding examples).  \n",
      "- **Tone**: Conversational with vivid analogies (\"spotlights,\" \"Swiss Army knife\").  \n",
      "- **Attribution**: Paper title/authors mentioned upfront; Google's permission for reuse included.  \n",
      "- **Flow**: Answers each question directly, linking technical innovation to real-world impact.\n"
     ]
    }
   ],
   "source": [
    "print(\"---- Thought Process ----\")\n",
    "print(raw_podcast_script.choices[0].message.content.split(\"</think>\")[0])\n",
    "print(\"\\n---- Actual Output ----\")\n",
    "print(final_podcast_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebec2be",
   "metadata": {},
   "source": [
    "### Audio Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d175a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio(podcast_script):\n",
    "    embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "\n",
    "    speaker_embeddings = embeddings_dataset[7306][\"xvector\"]\n",
    "    speaker_embeddings = torch.tensor(speaker_embeddings).unsqueeze(0)\n",
    "    pipe = pipeline(\"text-to-speech\", model=\"microsoft/speecht5_tts\")\n",
    "\n",
    "    # limiting to first 600 words, due to the model limitation\n",
    "    speech = pipe(podcast_script[:600], forward_params={\"speaker_embeddings\": speaker_embeddings})\n",
    "\n",
    "    return speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f52f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_audio = generate_audio(final_podcast_script)\n",
    "\n",
    "out_audio_path = \"../audio\"\n",
    "out_audio_file_name = in_file_name.split(\".\")[0] + \".wav\"\n",
    "out_audio_folder_path = f\"{out_audio_path}/{out_audio_file_name}\"\n",
    "\n",
    "sf.write(out_audio_folder_path, podcast_audio[\"audio\"], samplerate=podcast_audio[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da3a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27848a57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
